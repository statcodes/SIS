% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boot_sis.R
\name{boot_sis}
\alias{boot_sis}
\title{Calculates confidence intervals by using the quantile bootstrap approach.}
\usage{
boot_sis(
  x,
  y,
  family,
  penalty,
  sig = 0.05,
  covars,
  probs = c(0.1, 0.9),
  parallel = TRUE
)
}
\arguments{
\item{x}{The design matrix, of dimensions n * p, without an intercept. Each row is an observation vector.}

\item{y}{The response vector of dimension n * 1. Quantitative for
\code{family='gaussian'}, non-negative counts for \code{family='poisson'},
binary (0-1) for \code{family='binomial'}. For \code{family='cox'}, \code{y}
should be an object of class \code{Surv}, as provided by the function
\code{Surv()} in the package \pkg{survival}.}

\item{family}{Response type (see above).}

\item{penalty}{The penalty to be applied in the regularized likelihood
subproblems. 'SCAD' (the default), 'MCP', 'lasso', 'enet' (elastic-net), 'msaenet' (multi-step adaptive elastic-net) and
'aenet' (adaptive elastic-net) are provided.}

\item{sig}{significance threshold for the confidence interval}

\item{covars}{factor covariate names}

\item{probs}{Quantiles to compare for the effect estimation. By default quantiles are 10th and 90th}

\item{parallel}{Specifies whether to conduct parallel computing. If TRUE, it uses the parameter \code{parallel} from the \pkg{glmnet} package
to parallelize the computation}
}
\value{
Returns an object with 
\itemize{
\item{coef}{Coefficient}
\item{CI_low}{Lower confidence interval of the coefficient}
\item{CI_up}{Upper confidence interval of the coefficient}
\item{Est}{Effect estimate (mean difference for gaussian family, hazard ratio for Cox
family and odds ratio for binomial family) when comparing the two specified quantiles in \code{probs}}
\item{CI_low_perc}{Lower confidence interval of the effect estimate when comparing the quantiles specified in \code{probs}}
\item{CI_up_perc}{Upper confidence interval of the effect estimate when comparing the quantiles specified in \code{probs}}
}
}
\description{
The algorithm first conducts 10 repetitions of a 200 iterations
bootstrap. Then, it checks whether the standard error of the mean of those 10 estimations for both upper and lower confidence intervals is
lower than the 5 % of the length of the interval for each variable. If this condition is met for more than 95 % of the variables,
the bootstrap sample is adequate and we use the constructed sample to calculate upper and lower confidence intervals. If the condition is not met for more than 5 % of the variables,
then the variability is too high and we need to increase the number of bootstrap repetitions. Thus, the process of testing 200 bootstrap samples 10 times is repeated and added to the previous bootstrap estimates.
This process is repeated until the condition is met for more than 95 % of the variables.
}
\references{
Jerome Friedman and Trevor Hastie and Rob Tibshirani (2010)
Regularization Paths for Generalized Linear Models Via Coordinate Descent.
\emph{Journal of Statistical Software}, \bold{33}(1), 1-22.

Noah Simon and Jerome Friedman and Trevor Hastie and Rob Tibshirani (2011)
Regularization Paths for Cox's Proportional Hazards Model Via Coordinate
Descent. \emph{Journal of Statistical Software}, \bold{39}(5), 1-13.

Patrick Breheny and Jian Huang (2011) Coordiante Descent Algorithms for
Nonconvex Penalized Regression, with Applications to Biological Feature
Selection. \emph{The Annals of Applied Statistics}, \bold{5}, 232-253.

Hirotogu Akaike (1973) Information Theory and an Extension of the Maximum
Likelihood Principle. In \emph{Proceedings of the 2nd International
Symposium on Information Theory}, BN Petrov and F Csaki (eds.), 267-281.

Gideon Schwarz (1978) Estimating the Dimension of a Model. \emph{The Annals
of Statistics}, \bold{6}, 461-464.

Jiahua Chen and Zehua Chen (2008) Extended Bayesian Information Criteria for
Model Selection with Large Model Spaces. \emph{Biometrika}, \bold{95},
759-771.
}
\author{
Jianqing Fan, Yang Feng, Diego Franco Saldana, Richard Samworth, Arce Domingo-Relloso and Yichao Wu
}
